\documentclass[a4paper,12pt]{article}
\usepackage[utf8x]{inputenc}
\usepackage[numbers,sort&compress]{natbib}
\usepackage{amsfonts}
\usepackage{amsthm}
%\usepackage{fullpage}

\title{A Review of Recommender Systems using Collaborative Filtering}
\author{Pengfei Gao, s1144374}

\begin{document}

\maketitle

\begin{abstract}

The review presents an overview of the recent research area of recommender systems and describes the recent approaches based on the following three main types: content-based, collaborative filtering, and hybrid model. The review mainly focuses on the collaborative filtering approach and describes different algorithms implemented based on the two main models: memory-based model and model-based model. The review also introduces the current research on the time-drifting effects for the recommender systems.

\end{abstract}

\newpage

\tableofcontents

\newpage


\section{Introduction}


With the flourishing development of Internet, it is driven by the same belief in the unbounded opportunity to analyse the huge amount of data produced by people surfing the Internet. Online service providers and retailers are facing the challenges predicting users preferences and recommending the right, personalized contents that meet their tastes\citep{1167344, Das:2007:GNP,  Adomavicius:2005:TNG:1070611.1070751}. So recommender systems becomes an important issue in both research and industry areas. Recommender systems compares the items and users' information and find correlations among them, and provide a personalized and ranked items or content to users. 
\newline


\citep{Adomavicius:2005:TNG:1070611.1070751} presents an overview of current recommender systems strategies which are classified into three mainly approaches: content-based, collaborative filtering and hybrid models. The hybrid models combine the content-based approach and collaborative filtering and utilize them in different phases to avoid  specific limitations of these two approaches. In content-based approaches, it first creates a profile for each user which describes user's tastes, preferences and requirements estimated by user's registered information, feedback and past behaviours. Each item also has a profile which contains its categories and characteristics which are usually represented as keywords extracted from the content of itself \citep{Balabanovic:1997:FCC:245108.245124}. Some weighting measure may used to distinguish the importance of user and items' features \citep{Adomavicius:2005:TNG:1070611.1070751}.  Then calculate the relations among the users and items and recommend the best-matched items to users. For example, using cosine similarity measure to utilize the similar features among user and items' profile \citep{1167344}. However, \citep{Adomavicius:2005:TNG:1070611.1070751} indicates this approach require extracting features from items' contents which might not be easy.
\newline


The alternative approach to content-based approach is collaborative filtering which relies on users' past behaviours, such as item ratings and past transactions \citep{5197422}. Collaborative filtering learns predictive models of user preferences and behaviours from historic data which is content domain free and avoids large amount of data collection\cite{Hofmann:2003:CFV:860435.860483}. \citep{Koren:2008:FMN:1401890.1401944} indicates that it allows complicated and unknown data patterns which might be difficult to characterise using known data features.
\newline


\section{Collaborative Filtering}
\citep{BreeseHK98} identifies two general classes for collaborative filtering: memory-based (also known as neighborhood models \citep{Koren:2008:FMN:1401890.1401944, Hofmann:2003:CFV:860435.860483}) and model-based. Memory-based methods are centred on calculating the relationships between users or items maintained in a database and the computation is performed across the whole database \citep{PennockHLG00}. For example, an user-oriented approach predicts the rating $ \hat{r}  $ for user $ i $ to an item $ j $ based on the rating made by other users on the same item $j$.  \cite{BreeseHK98} presents several functions to explain this approach in details. Suppose $ S $ is the set of items and $n$ is the total number of items set $S$ that the user has rated before, then we can get the mean rating for the user u as:$$ \bar{r}_{i} = \frac{1}{n}\sum_{j\in S} r_{i,j} \eqno{(1)} $$ We then can calculate the predicted ratings for user $i$ based on some extra information about the user and some weights derived according to the user's preferences, profiles, and past behaviours. So for user $i$, we predict the rating $\hat{r}_{i,j}$ the user may rate on item $j$ is computed as: $$ \hat{r}_{i,j} = \bar{r}_{i} + \kappa\sum_{t=1}^{n}w(i,t)(r_{t,j} - \bar{r}_{i}) \eqno{(2)} $$ where $n$ is the number of all users and $t$ represents for each user in the sum loop. The weight function $w(i,t)$ may indicate distance, correlation, or similarity between other users and the current user $i$ \citep{BreeseHK98}. $\kappa$ is a normalizing constant which can help the weights' absolute values sum to unity  \citep{BreeseHK98}. There are many ways to define the weight functions. One of the most famous statistical way was first introduced in \cite{Resnick:1994:GOA:192844.192905} based on the Pearson correlation coefficient. The correlation between user $i$ and user $t$ is calculated as: $$ w(i,t) = \frac{\sum_{j\in S}(r_{i,j} -\bar{r}_{i})(r_{t,j}-\bar{r}_{t})}{\sqrt{\sum_{j\in S}(r_{i,j}-\bar{r}_{i})^{2}\sum_{j\in S}(r_{t,j}-\bar{r}_{t})^{2} }}  \eqno{(3)} $$ where the item $j$ was rated both by user $i$ and user $j$ before. The weights scales between $-1$ and $1$ indicating how similar of the ratings made by user $i$ and user $t$ on the same items.
\newline



Memory-based methods are very popular and have been used in many areas, for example GroupLens project\cite{Resnick:1994:GOA:192844.192905} and Amazon\cite{1167344}, because of its simplicity and intuitive process which avoid from building data analysis and modelling stage\cite{Hofmann:2003:CFV:860435.860483}. However, it is relying on a few specific  neighbourhood relationships such as the above user-oriented approach which ignored the characteristics and uniqueness of a specific user to distinguish with other users. Hence, nothing is really learned from the existed users' profiles, preferences and past behaviours which may cause problems when adding new items having no ratings  or new users haven't rating anything now. Moreover, for massive datasets, memory-based methods relies on the expensive offline computation which will result in the bad end-user experience for online users.
\newline


Model-based approaches have overcomes the drawbacks of memory-based methods. This method first tries to understand users from their profiles, preferences and past behaviours and cluster them into different models, and then calculate the predicted ratings for users on new items. \citep{BreeseHK98} presents a probabilistic model for collaborative filtering using a Bayesian classifier to cluster ratings depend on some given information. Users who have common interests and tastes may rating same items in a similar way and those users may be grouped into a specific class. Suppose the ratings are discrete number in a limited range, we set the range is from $0$ to $m$ here: $$ p_{i,j} = E(r_{i,j}) = \sum_{k=0}^{m}P(r_{i,j}=k|r_{i,l},l\in S_{i})\times k  \eqno({4}) $$ where $p_{i,j}$ is the probability of a particular rating for user $i$ may rate on the item $j$ given the user's past recorded ratings $r_{i,l},l\in S_{i}$, $S_{i}$ is the item set containing all the items that user $i$ has rated before. \citep{Ungar98clusteringmethods} describes a probabilistic model using cluster methods to separate both users and items into different clusters, such as EM algorithms, repeated K-means clustering, and Gibbs sampling. \citep{BreeseHK98} proposes a naive Bayes method to assign users $c$ into different clusters $C$: $$ P(C=c, r_{1},...,r_{n}) = P(C=c)\prod_{i=1}^{n}P(r_{i}|C=c)  \eqno({5}) $$ where $r_{1},...,r_{n}$ is the existed ratings they may rate on $1,...,n$ items and $P(C=c, r_{1},...,r_{n})$ is the probability of users belonging to the particular cluster $c$. The conditional probabilities $P(r_{i}|C=c)$ is estimated based on the ratings made by other users belonging to the cluster $c$. After that, \citep{BreeseHK98} uses Cheeseman and Stutz (1995) to find the efficient approximations for the marginal likelihood to assign the users or items to the particular clusters. 
\newline

\section{Probabilistic Latent Semantic Analysis}

Hofmann proposes a statistical approach for collaborative filtering named probabilistic Latent Semantic Analysis (pLSA) \citep{Hofmann:2004:LSM,Hofmann:1999:LCM,Hofmann:1999}. It first make user-item pairs $(i,j)$ of all possible users $i \in U$ and items $j \in S$ in the databases. Then hidden variables $Z$ are introduced to describe the relationships derived by modelling the joint distribution of users and items as a mixture distribution \citep{Das:2007:GNP}. Hofmann wrote the mixture model as the following equation: $$ p(j|i; \theta ) = \sum_{z}P(j|z)P(z|i) $$ The parameters $\theta$ represent the conditional probabilities $p(z|i)$ and $p(j|z)$. The hidden variables $Z$ are used to represent a reason for user $i$ to prefer a item $j$. Each state $ z \in Z $ is supposed to explain the unknown reason that can not be observed. So we can separate users into related user clusters and items into the related clusters based on those hidden variables $Z$. Then Hofmann estimates the model parameters $\theta$ such that the product of conditional log-likelihood is maximized: 
$$ L(\theta ) = -\frac{1}{N} \sum_{n=1}^{N} log P(j_{n}|i_{n};\theta ) $$ 
where $N$ is the size of user-item pairs. EM algorithm is used to estimate the maxmized log-likehood parameters. The E-Step computes the optimal $Q$ , the posterior probabilities of the latent variables $\hat{\theta }$, given the parameter $\hat{\theta }$: 
$$ Q^{*}(z;i,j;\hat{\theta }) = P(z|i,j;\hat{\theta }) = \frac{\hat{P}(j|z)\hat{P}(z|i)}{ \sum_{z' \in Z} \hat{P}(j|z')\hat{P}(z'|i) } $$ 
The M-Step is for the optimization of the parameters $\theta$: 
$$ P(j|z) = \frac{ \sum_{i}Q^{*}(z;i,j;\hat{\theta }) }{ \sum_{j}\sum_{i'}Q*(z;;i,j,\hat{\theta }) } $$
$$ P(z|i) = \frac{ \sum_{j}Q^{*}(z;i,j;\hat{\theta })  }{ \sum_{z'}\sum_{j}Q^{*}(z';i,j;\hat{\theta }) } $$
\citep{Das:2007:GNP} introduces a way to perform the EM computation paralleled and scalable using the MapReduce model \citep{Dean:2008:MSD:1327452.1327492}. 
\newline





\citep{Adomavicius:2005:TNG:1070611.1070751} points out a limitation of this method that each user will be assigned to the most likely cluster while the user may interest totally different and unrelated topics such as driving and reading. 




\section{Singular Value Decomposition}


Due to the expensive computation cost of the huge increasing amount of data and the sparsity of the data becomes a series problem, lots of dimensionality reduction methods has been investigated both in the research and industry areas to address the problems. \citep{Billsus:1998:LCI:645527.657311} proposes a approach using Singular Value Decomposition(SVD). First \citep{Billsus:1998:LCI:645527.657311} constructs a big user-item matrix which contains boolean values indicating whether the user has rated the items. In fact, the matrix is really sparse because there may be millions of items while a user may only rate dozens of them. In other words, the majority values in the matrix will be zero. SVD is a famous matrix factorization method that will can decompose a $m\times n$ matrix $A$ into the product of three matrices as following: $$ A = U \cdot \sum \cdot V^{T}  $$ where $U$ is a $m \times r$ matrix and $V$ is a $n \times r$ matrix; $ r $ is the rank of the diagonal matrix $\sum$ which contains the singular values. The numerical values of those singular values reflects the impact they can affect how much captured of the original matrix $A$. For example, in a particular situation, $k$ largest singular values with the corresponding singular vectors in the matrices of $U$ and $V$ can recover the important "latent" structure of the original matrix $A$ \citep{Billsus:1998:LCI:645527.657311}. Therefore, we can reduce the the size of singular values from $r$ to $k$. Moreover, the user ratings and items can represented in k-dimensional space. 
\newline



\citep{Sarwar00applicationof} presents two experiments on dimensionality reduction in recommender systems comparing the quality using SVD and collaborative filtering. They choose Mean Absolute Error (MAE) as the evaluation metric to compare these two methods. The result shows that SVD methods is much better and can provide better online recommendation performance than collaborative filter.
\newline



\section{Explicit and Implicit Feedback}
Recommender systems make predictions based on the different kinds of input. \citep{4781121} indicates the high quality explicit feedback which directly reflects users' preference on the items contribute the most convenient to the recommending process. For example, Netflix records the star ratings for movies and Youtube uses "Like" and "Dislike" buttons as a vote for the items. However, in some practical situations, explicit feedback is not enough for an accuracy recommender system. For example, Google News \citep{Das:2007:GNP} treats a user's click on an article as a positive vote. But, regarding a click as a positive vote is much more noisy than the Netflix explict 1-5 star ratings. For a web page within lots of news links, a user may make many wrong clicks on those news which the user do not want to read. What's more, as the clicks can be regarded as a user's interest, they can not say anything about a user's negative interest. Thus, derive users preference from users' implicit feedback such as observing user behaviours is a very important way to improve the accuracy of the recommender systems  \citep{Oard_Kim_1998}.  \citep{4781121} lists 4 types of the implicit feedback such as purchase history, browsing history, search patterns, and even mouse movements. After collecting the feedback, the issue becomes how to process them. \citep{4781121} indicates that it is very hard to identify the unique features of those various implicit feedback collected from various approaches directly using the algorithms designed for the explicit feedback. Google News uses a covisitation instances \citep{Das:2007:GNP} to process the user clicks. Whenever Google receive a click from the user $i$ on an article $j$, they will search the user's recent click history $C_{i}$ and iterate over the items in it. For such articles $k \in C_{i} $, they create a pair $(j,k)$ and add a weighted adjacency distance to the pair. If the pair has already been created, they will update the age discounted count. So for a given article $j$, its near neighbours are effectively the set of articles the user want to view.





\section{Hybrid model}

From the above, we can see that memory-based methods are better at modelling the neighbour relationships and really effective among the very small localized system while model-based methods are better at estimate the whole datasets and perform better for the larger scale systems.
The Netflix prize has attracted a great of attention on the Collaborative Filtering area since 2006. With no team reached the perfect requirements of the prize, the prize was awarded the annual progress prize to the KorBell team which achieved the most outstanding improvement so far \citep{Bell:2007:LNP}. Koren, one of the two members in the KorBell team, proposed a hybrid model that improve accuracy and efficiency by utilizing the advantages of both memory-based and model-based methods. In \citep{Bell:2007:LNP, Bell:2007:MRM:1281192.1281206, 4470228, 4781121}, Koren et al. present a baseline estimates to adjust the effects caused by some users who made higher ratings than other users and some items that are more popular than other items as following:
$$ b_{i,j} = \mu + b_{i} + b_{j}  \eqno({6})$$
where $b_{i}$ and $b_{j}$ are the averages recorded ratings user $i$ made and the item $j$ received, respectively. $b_{i,j}$ is the baseline estimate predicted by the system for user $i$ on the item $j$. Therefore, for the memery-based model, equation (2) is changed to the following form:
$$ \hat{r}_{i,j} = b_{i,j} + \sum_{t \in S_{i}}(r_{i,j} - b_{i,j})w_{i,j} + \sum_{t \in N_{i}} c_{j,t} $$ 
where $c_{j,t} $ is the global offsets similar the weight factor $w_{j,t}$ from item $j$ to item $t$ \citep{Bell:2007:LNP}. $N_{i}$ is the implicit feedback observed from user $i$ behaviour, for example, user $i$ has searched the item for twenty times while he rarely search any other items more than three times. In order to predict faster and more specific recommendations, and also avoid overfitting problems for new users who rarely rate, Koren moderates the equation:
$$ \hat{r}_{i,j} = b_{i,j} + |R^{k}_{(j;i})|^{-\frac{1}{2}} \sum_{t \in R^{k}_{(j;i)}}(r_{i,t}-b_{i,t})w_{j,t} + |N^{k}_{(j;i)}|^{-\frac{1}{2}}\sum_{t \in N^{k}_{(j;i)}}c_{j,t}    \eqno({7}) $$
where $ R^{k}_{(j;i)} = R_{i} \cap S^{k}_{j} $ and $ N^{k}_{(j;i)} = N_{i} \cap S^{k}_{j} $. $ S^{k}_{j} $ is the dataset of $k$ items that are most similar to item $j$. 
\newline


For model-based methods, \citep{Koren:2008:FMN:1401890.1401944} presents a matrix factorization models which is much similar to the SVD for identify latent factors of the ratings matrix as following:
$$ \hat{r}_{i,j} = b_{i,j} + p_{i}^{T}q_{i} \eqno({8}) $$
where $f$ is the dimension of a joint latent factor space. Each user $i$ links to a user-factors vector $p_{i} \in  \mathbb{R}^{f}$ and each item $j$ to an item-factors vector $q_{j} \in \mathbb{R}^{f} $. \citep{5197422} indicates that for the given user $i$, the factors of $p_{i}$ measure the degree of the user prefers the items on the relating factors. The above equation reveals the interaction between the user $i$ and the item $j$ - the user's preference for the item's features. Following the gradient descent optimization performed in \citep{Paterek_2007}, \citep{Koren:2008:FMN:1401890.1401944} proposes a rule:
$$ \hat{r}_{i,j} = b_{i,j} + q_{j}^{T} ( |R_{i}|^{-\frac{1}{2}} \sum_{t \in R_{i}} (r_{i,t} - b_{i,t})x_{t} + |N_{i}|^{-\frac{1}{2}} \sum_{t \in N_{i}}y_{t} )   \eqno({9}) $$
where $ q_{j},x_{t},y_{t} \in \mathbb{R}_{f} $ are factor vectors relating to the each item $j$. In the other way, we can define users using the items they intrested in without providing the explicit parameterization for users. Therefore, the previous user factor $p_{j}$ can be replaced by $  |R_{i}|^{-\frac{1}{2}} \sum_{t \in R_{i}} (r_{i,t} - b_{i,t})x_{t} + |N_{i}|^{-\frac{1}{2}} \sum_{t \in N_{i}}y_{t} $. \citep{Koren:2008:FMN:1401890.1401944} called this model "Asymmetric-SVD". This new model reduces the parameters from the number of users to the number of items that the user prefer recorded by both explicit and implicit feedback. Thus, it lowers the complexity of the computation. The second, since there is no parameterization for users in the Asymmetric-SVD model, the system will recommend services to users instantly once they provide any kind feedback. In other words, the system can immediately utilize the new ratings to update user preferences. The third, \citep{Herlocker:2000:ECF} indicates the fact that a user may be willing to risk buy a book based on the recommendation systems while he will not risk choosing a vacation travel recommended by such a system. The reason is that the system make recommendations based on the computation of large amount of sparse and incomplete data in a black box. There is no explanation capabilities to persuade users to belief in. Users will not trust the system unless they know the reasons why the system recommend the items. However, the predictions made by the Asymmetric-SVD model are straightforwardly based on users' past feedback. Moreover, the model can allow the user to identify which of the other user's past feedback influence the predictions most. Koren combines these two models: the memory-based model and the model-based model, by summing the equations of $(6)$ and $(7)$. Therefore, these two models nicely complement each other as follows:
$$ \hat{r}_{i,j} = b_{i,j} +  q_{j}^{T} ( |R_{i}|^{-\frac{1}{2}} \sum_{t \in R_{i}} (r_{i,t} - b_{i,t})x_{t} + |N_{i}|^{-\frac{1}{2}} \sum_{t \in N_{i}}y_{t} )    $$
$$+|R^{k}_{(j;i})|^{-\frac{1}{2}} \sum_{t \in R^{k}_{(j;i)}}(r_{i,t}-b_{i,t})w_{j,t}  
+ |N^{k}_{(j;i)}|^{-\frac{1}{2}}\sum_{t \in N^{k}_{(j;i)}}c_{j,t}   \eqno({10}) $$
The above equation presents a 3-level model for predictions. The first level, $b_{i,j}$ (6), describes the normal properties of the user and item without considering any relations between them. For example, ipod is an electronic product and the user's rating scale is the average. The second level, $ q_{j}^{T} ( |R_{i}|^{-\frac{1}{2}} \sum_{t \in R_{i}} (r_{i,t} - b_{i,t})x_{t} + |N_{i}|^{-\frac{1}{2}} \sum_{t \in N_{i}}y_{t} )  $ considers the relation between the user properties and item properties. For example, ipod receives good ratings and the user prefer rating high on new and cool electronic products. The last neighbourhood level is focus on the grained adjustments that is special and not easy found on user's  properties. For example, the user rated very low on all the products made by Apple. Above all, the Asymmetric-SVD model presents a straightforwardly explanation on the predictions made by the system and avoid the re-training the model for adding new items or users. The explainability will win the confidence of the users and the hybrid model improves the accuracy and efficiency by addressing different features of the original data.

\section{Temporal Dynamics}
So far, the models describes here are all static. In reality, items' perception and popularity change constantly due to the changes external circumstances. For example, Sony Walkman was very popular but now it was disappearing as electronic music player becomes the mainstream of the market. Similarly, customers' inclinations evolve leads people to redefine their preferences and interests. Thus, recommendation should involve the temporal effects which reflect the time-drifting, dynamic nature of user-items interactions \citep{Ding:2005:TWC:1099554.1099689}. Koren \citep{Koren:2010:CFT:1721654.1721677} proposes a method to model the time changing behaviour over the life span of the original data and applies to the recommender systems based on the matrix factorization approach \citep{5197422}. The method models temporal effects by decomposing user ratings into distinct aspects which vary over time: item biases $b_{j}(t)$, user biases $b_{i}(t)$, and user preferences $p_i(t)$. The item biases $b_{j}(t)$ reflects the item $j$ popularity variation over time and the user biases $b_{i}(t)$ reflects user $i$ rating baselines variation over time. The user preferences $p_i(t)$ reflects the changes of user's preferences over time. So, the exact parametrizations of time-drifting parameters change the equation (6) and equation (8) into the following: 
$$ \hat{r}_{i,j}(t) = \mu + b_{j}(t) + b_{i}(t) + q_{j}^{T}p_{i} \eqno({11}) $$
Koren applied the method into collaborative filtering recommender approaches and the results proved the temporal dynamic very useful to improve the quality of the recommendations \citep{Koren:2010:CFT:1721654.1721677} .





\section{Conclusion}

As described above, there has been lots research done in both academic and industry areas on the recommender systems during the past years. A broad range of techniques such as statistical, information retrieval, machine learning and others have been applied to the recommender systems. As shown before, the current recommender systems can be categorized into three main types: content-based, collaborative filtering, and hybrid model. Content-based methods classify a series of discrete features of the items in order to make recommendations which share the similar properties. The collaborative filtering model focus on learns the predictive models of user preferences from the historic data, such as users' past behaviours and profiles. 
\newline

Memory-based and model-based models are the two general classes for collaborative filtering. Memory-based methods focus on calculating the localized relationships between users and items and the computation is performed across the whole database. Memory-based methods are very popular because of its simplicity and intuitive process which avoid from building data analysis and modelling stage. The expensive computation over the whole dataset and the ignorance the characteristics and uniqueness of a specific user form other users. Model-based methods try to build models for users from their profiles, preferences and past behaviours and cluster them into different classes. The model-based techniques includes naive Bayesian clustering techniques, Bayesian networks, dependency networks and matrix factorization. In the real life the users' feedback has been classified into two classes. One is the high quality explicit feedback and the other is the implicit feedback collected by such as observing user behaviours. As both memory-based and model-based methods has their own limitations, the hybrid approach by combining the these two models can help to improve the accuracy and efficiency by utilizing the strengths of both them. The review introduces the matrix factorization approach proposed by Koren et al. \citep{5197422} in details. The complex factor dimensionality which descriptions contains more distinct parameters can help to improve the accuracy. Moreover, the experience such as Netflix Prize \citep{Bell:2007:LNP} has shown that matrix factorization approach performance is beyond the classical recommendation techniques. At last, the review briefly introduces the temporal effects which reflect the time-drifting nature of user-items interactions and the methods modelling the time changing behaviour over the life span of the original data.
\newline 

Nowadays, most research on the recommender systems has focused on improving the accuracy of the algorithms. However, being accurate is not enough \citep{McNee:2006:AEA}. Moreover, the testing and evaluation work of the most recommender systems is based on the historic data, how to find a meaningful and credible recommendation for users is more important and valuable than the accuracy. The research of recommender systems should do more from a user-centric perspective not only efficient and accurate but also meaningful and trustful.



































\bibliographystyle{plain}
\bibliography{refs}


\end{document}